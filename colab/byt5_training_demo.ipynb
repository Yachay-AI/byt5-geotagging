{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Install required packages\n",
        "!pip install transformers==4.29.1 tqdm==4.63.2 pandas==1.4.4 wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jxQONAqxzOE",
        "outputId": "50233d1b-db4d-4bea-ad25-24a58ec2ce2a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.29.1\n",
            "  Downloading transformers-4.29.1-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm==4.63.2\n",
            "  Downloading tqdm-4.63.2-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.6/76.6 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas==1.4.4\n",
            "  Downloading pandas-1.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb\n",
            "  Downloading wandb-0.15.12-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.1) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers==4.29.1)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.1) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.1) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.1) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.1) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.29.1) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.29.1)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.4.4) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.4.4) (2023.3.post1)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.37-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.0/190.0 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.31.0-py2.py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.8/224.8 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.1) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.29.1) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.1) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.1) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.29.1) (2023.7.22)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=0f69ab462999f6620be9737e56a0a6562f14aafcc43b53924b4f5d88417739b6\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: tokenizers, pathtools, tqdm, smmap, setproctitle, sentry-sdk, docker-pycreds, pandas, huggingface-hub, gitdb, transformers, GitPython, wandb\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.1\n",
            "    Uninstalling tqdm-4.66.1:\n",
            "      Successfully uninstalled tqdm-4.66.1\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 1.4.4 which is incompatible.\n",
            "plotnine 0.12.3 requires pandas>=1.5.0, but you have pandas 1.4.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.37 docker-pycreds-0.4.0 gitdb-4.0.10 huggingface-hub-0.17.3 pandas-1.4.4 pathtools-0.1.2 sentry-sdk-1.31.0 setproctitle-1.3.3 smmap-5.0.1 tokenizers-0.13.3 tqdm-4.63.2 transformers-4.29.1 wandb-0.15.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sa4vndBrimra",
        "outputId": "aa1f42d3-8659-4154-dd9e-8c6eba037e46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "n2EsT4lghapg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1803c302-a682-4e77-c66f-40a391699bcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'byt5-geotagging'...\n",
            "remote: Enumerating objects: 244, done.\u001b[K\n",
            "remote: Counting objects: 100% (98/98), done.\u001b[K\n",
            "remote: Compressing objects: 100% (86/86), done.\u001b[K\n",
            "remote: Total 244 (delta 54), reused 18 (delta 10), pack-reused 146\u001b[K\n",
            "Receiving objects: 100% (244/244), 13.03 MiB | 19.49 MiB/s, done.\n",
            "Resolving deltas: 100% (100/100), done.\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (4.6.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown) (3.12.4)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown) (1.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from gdown) (4.63.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (2023.7.22)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Downloading...\n",
            "From: https://drive.google.com/u/2/uc?id=1thkE-hgT3sDtZqILZH17Hyayy0hkk_jh\n",
            "To: /content/challenge_1.tar.gz\n",
            "100% 107M/107M [00:02<00:00, 45.9MB/s] \n",
            "{\"text\": \"le estará afectando la menopáusia? https://t.co/ij2y1j53c0\", \"coordinates\": [\"-78.77473262476528\", \"-2.199325289082757\"]}\n",
            "{\"text\": \"acaba de publicar una foto en conocoto, pichincha, ecuador https://t.co/qwrmyfex0a\", \"coordinates\": [\"-78.1677027989582\", \"-0.5747879967862987\"]}\n",
            "{\"text\": \"acaba de publicar una foto en volcán cotacachi https://t.co/vbcw4bfpgn\", \"coordinates\": [\"-78.70883119538775\", \"0.5082920403995214\"]}\n",
            "{\"text\": \"@karlosivan3 sonó gay eso\", \"coordinates\": [\"-80.94486768374395\", \"-2.198928662948922\"]}\n",
            "{\"text\": \"#naranjal || 🇮🇹 amores de mis amores, mis viejitos, #mamicristina ❤ &amp; #papichachita ❤ https://t.co/ouqagwyvfs\", \"coordinates\": [\"-78.81327244010579\", \"-2.74083466883138\"]}\n",
            "{\"text\": \"clever ruiz alcalde dist fernando lores https://t.co/6jffvozlft\", \"coordinates\": [\"-72.99913644420953\", \"-4.367472282836016\"]}\n",
            "{\"text\": \"＂ las posibildades que tenemos para dar una mano y ser amigos son infinitas ＂créditos: @joanflorestrux en templo de trujillo perú https://t.co/xnwwt1vnez\", \"coordinates\": [\"-78.57473722727678\", \"-8.155904341067638\"]}\n",
            "{\"text\": \"@otraperiodista es el motor de todo, a pesar de 💔💔estar roto y descosido ,sigo pensando que es la esencia de la vida .\", \"coordinates\": [\"-76.55206046997611\", \"1.0500068334392538\"]}\n",
            "{\"text\": \"dios los bendiga amigos, gracias por apoyar esta labor que amo #soylavoluntariaintensa 🧡🤲🏽🧡🙏🏽🤡 #laparejadelflow https://t.co/jqfj87x7mo\", \"coordinates\": [\"-78.81327244010579\", \"-2.74083466883138\"]}\n",
            "{\"text\": \"cómo estamos en campaña política nadie dice nada viva la fiesta 🤷‍♂️ https://t.co/qnk6gndag5\", \"coordinates\": [\"-79.85980022588167\", \"-2.1991269670656575\"]}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Clone the repository for geotagging with BYT5\n",
        "!git clone https://github.com/Yachay-AI/byt5-geotagging\n",
        "\n",
        "\n",
        "# Install gdown for Google Drive downloads\n",
        "!pip install gdown\n",
        "\n",
        "# Download the dataset\n",
        "!gdown https://drive.google.com/u/2/uc?id=1thkE-hgT3sDtZqILZH17Hyayy0hkk_jh&export=download\n",
        "\n",
        "# Unzip the downloaded dataset\n",
        "!tar xvf challenge_1.tar.gz > /dev/null\n",
        "\n",
        "# Read sample data\n",
        "!head data_sample_lc/c_46.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ad1kJoI4hcVB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "749f0f78-0524-4d0a-9bdb-f62bc4b3986c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "731994 train.csv\n",
            "81623 test.csv\n"
          ]
        }
      ],
      "source": [
        "# Import necessary modules\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Initialize an empty list to hold dataframes\n",
        "df_list = []\n",
        "\n",
        "# Loop through each JSON file and append it to df_list\n",
        "for fn in os.listdir(\"data_sample_lc\"):\n",
        "  df_list.append(pd.read_json(f\"data_sample_lc/{fn}\", lines=True))\n",
        "\n",
        "# Concatenate all dataframes in df_list\n",
        "df = pd.concat(df_list)\n",
        "\n",
        "# Extract latitude and longitude from 'coordinates' column\n",
        "df['lat'] = [x[1] for x in df['coordinates']]\n",
        "df['lon'] = [x[0] for x in df['coordinates']]\n",
        "\n",
        "# Drop the 'coordinates' column\n",
        "df.drop('coordinates', axis=1, inplace=True)\n",
        "\n",
        "# Shuffle the dataset\n",
        "df = df.sample(frac=1.0)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "df.iloc[:len(df)*9//10].to_csv('train.csv')\n",
        "df.iloc[len(df)*9//10:].to_csv('test.csv')\n",
        "\n",
        "# Count the number of lines in train.csv and test.csv\n",
        "!wc -l train.csv\n",
        "!wc -l test.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7swmfDechfIn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load cluster data\n",
        "cluster_df = pd.read_csv('byt5-geotagging/cluster_df.csv')\n",
        "\n",
        "# Save the clustering model\n",
        "with open('clustering.pkl', 'wb') as fout:\n",
        "  pickle.dump((cluster_df, []), fout)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZhJc7tshgcv",
        "outputId": "4908262e-a0df-41e8-f9a1-ae17eeb3c3f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:root:start\n",
            "start\n",
            "INFO:root:finish reading test file\n",
            "finish reading test file\n",
            "Warning, dropping 1 NaN rows\n",
            "INFO:root:finish reading train file\n",
            "finish reading train file\n",
            "INFO:root:Index(['Unnamed: 0', 'text', 'lat', 'lon', 'coordinates'], dtype='object')\n",
            "Index(['Unnamed: 0', 'text', 'lat', 'lon', 'coordinates'], dtype='object')\n",
            "Downloading (…)okenizer_config.json: 100% 2.59k/2.59k [00:00<00:00, 11.9MB/s]\n",
            "Downloading (…)lve/main/config.json: 100% 698/698 [00:00<00:00, 3.14MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 2.50k/2.50k [00:00<00:00, 10.7MB/s]\n",
            "Downloading pytorch_model.bin: 100% 1.20G/1.20G [00:39<00:00, 30.4MB/s]\n",
            "Some weights of the model checkpoint at google/byt5-small were not used when initializing T5EncoderModel: ['decoder.block.1.layer.0.layer_norm.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.2.layer.2.DenseReluDense.wo.weight', 'lm_head.weight', 'decoder.block.2.layer.0.SelfAttention.v.weight', 'decoder.block.1.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.0.SelfAttention.o.weight', 'decoder.block.3.layer.0.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.q.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.2.layer.0.SelfAttention.q.weight', 'decoder.block.0.layer.0.SelfAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.0.layer.1.EncDecAttention.o.weight', 'decoder.block.2.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.0.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.0.SelfAttention.k.weight', 'decoder.block.3.layer.1.EncDecAttention.k.weight', 'decoder.block.0.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.3.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.o.weight', 'decoder.block.1.layer.2.layer_norm.weight', 'decoder.block.3.layer.2.layer_norm.weight', 'decoder.block.0.layer.1.layer_norm.weight', 'decoder.block.1.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.2.DenseReluDense.wo.weight', 'decoder.block.3.layer.1.EncDecAttention.o.weight', 'decoder.block.1.layer.0.SelfAttention.k.weight', 'decoder.block.1.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight', 'decoder.block.3.layer.1.EncDecAttention.q.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.1.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.2.layer_norm.weight', 'decoder.block.0.layer.1.EncDecAttention.v.weight', 'decoder.block.3.layer.0.SelfAttention.q.weight', 'decoder.embed_tokens.weight', 'decoder.block.3.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.2.DenseReluDense.wo.weight', 'decoder.block.2.layer.1.EncDecAttention.k.weight', 'decoder.block.2.layer.1.layer_norm.weight', 'decoder.block.2.layer.1.EncDecAttention.v.weight', 'decoder.block.0.layer.1.EncDecAttention.k.weight', 'decoder.block.1.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.2.DenseReluDense.wi_1.weight', 'decoder.block.0.layer.0.layer_norm.weight', 'decoder.block.1.layer.1.layer_norm.weight', 'decoder.block.3.layer.2.DenseReluDense.wi_0.weight', 'decoder.block.3.layer.0.SelfAttention.v.weight', 'decoder.block.0.layer.0.SelfAttention.q.weight', 'decoder.block.1.layer.1.EncDecAttention.k.weight', 'decoder.block.3.layer.1.layer_norm.weight', 'decoder.block.1.layer.1.EncDecAttention.o.weight', 'decoder.final_layer_norm.weight', 'decoder.block.2.layer.0.SelfAttention.o.weight', 'decoder.block.2.layer.1.EncDecAttention.o.weight', 'decoder.block.0.layer.2.layer_norm.weight', 'decoder.block.0.layer.0.SelfAttention.v.weight', 'decoder.block.2.layer.1.EncDecAttention.q.weight', 'decoder.block.2.layer.0.layer_norm.weight']\n",
            "- This IS expected if you are initializing T5EncoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing T5EncoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "INFO:root:ByT5_classifier(\n",
            "  (byt5): T5EncoderModel(\n",
            "    (shared): Embedding(384, 1472)\n",
            "    (encoder): T5Stack(\n",
            "      (embed_tokens): Embedding(384, 1472)\n",
            "      (block): ModuleList(\n",
            "        (0): T5Block(\n",
            "          (layer): ModuleList(\n",
            "            (0): T5LayerSelfAttention(\n",
            "              (SelfAttention): T5Attention(\n",
            "                (q): Linear(in_features=1472, out_features=384, bias=False)\n",
            "                (k): Linear(in_features=1472, out_features=384, bias=False)\n",
            "                (v): Linear(in_features=1472, out_features=384, bias=False)\n",
            "                (o): Linear(in_features=384, out_features=1472, bias=False)\n",
            "                (relative_attention_bias): Embedding(32, 6)\n",
            "              )\n",
            "              (layer_norm): T5LayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (1): T5LayerFF(\n",
            "              (DenseReluDense): T5DenseGatedActDense(\n",
            "                (wi_0): Linear(in_features=1472, out_features=3584, bias=False)\n",
            "                (wi_1): Linear(in_features=1472, out_features=3584, bias=False)\n",
            "                (wo): Linear(in_features=3584, out_features=1472, bias=False)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "                (act): NewGELUActivation()\n",
            "              )\n",
            "              (layer_norm): T5LayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (1-3): 3 x T5Block(\n",
            "          (layer): ModuleList(\n",
            "            (0): T5LayerSelfAttention(\n",
            "              (SelfAttention): T5Attention(\n",
            "                (q): Linear(in_features=1472, out_features=384, bias=False)\n",
            "                (k): Linear(in_features=1472, out_features=384, bias=False)\n",
            "                (v): Linear(in_features=1472, out_features=384, bias=False)\n",
            "                (o): Linear(in_features=384, out_features=1472, bias=False)\n",
            "              )\n",
            "              (layer_norm): T5LayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (1): T5LayerFF(\n",
            "              (DenseReluDense): T5DenseGatedActDense(\n",
            "                (wi_0): Linear(in_features=1472, out_features=3584, bias=False)\n",
            "                (wi_1): Linear(in_features=1472, out_features=3584, bias=False)\n",
            "                (wo): Linear(in_features=3584, out_features=1472, bias=False)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "                (act): NewGELUActivation()\n",
            "              )\n",
            "              (layer_norm): T5LayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (final_layer_norm): T5LayerNorm()\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (fc3): Linear(in_features=1472, out_features=3000, bias=True)\n",
            ")\n",
            "ByT5_classifier(\n",
            "  (byt5): T5EncoderModel(\n",
            "    (shared): Embedding(384, 1472)\n",
            "    (encoder): T5Stack(\n",
            "      (embed_tokens): Embedding(384, 1472)\n",
            "      (block): ModuleList(\n",
            "        (0): T5Block(\n",
            "          (layer): ModuleList(\n",
            "            (0): T5LayerSelfAttention(\n",
            "              (SelfAttention): T5Attention(\n",
            "                (q): Linear(in_features=1472, out_features=384, bias=False)\n",
            "                (k): Linear(in_features=1472, out_features=384, bias=False)\n",
            "                (v): Linear(in_features=1472, out_features=384, bias=False)\n",
            "                (o): Linear(in_features=384, out_features=1472, bias=False)\n",
            "                (relative_attention_bias): Embedding(32, 6)\n",
            "              )\n",
            "              (layer_norm): T5LayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (1): T5LayerFF(\n",
            "              (DenseReluDense): T5DenseGatedActDense(\n",
            "                (wi_0): Linear(in_features=1472, out_features=3584, bias=False)\n",
            "                (wi_1): Linear(in_features=1472, out_features=3584, bias=False)\n",
            "                (wo): Linear(in_features=3584, out_features=1472, bias=False)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "                (act): NewGELUActivation()\n",
            "              )\n",
            "              (layer_norm): T5LayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (1-3): 3 x T5Block(\n",
            "          (layer): ModuleList(\n",
            "            (0): T5LayerSelfAttention(\n",
            "              (SelfAttention): T5Attention(\n",
            "                (q): Linear(in_features=1472, out_features=384, bias=False)\n",
            "                (k): Linear(in_features=1472, out_features=384, bias=False)\n",
            "                (v): Linear(in_features=1472, out_features=384, bias=False)\n",
            "                (o): Linear(in_features=384, out_features=1472, bias=False)\n",
            "              )\n",
            "              (layer_norm): T5LayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "            (1): T5LayerFF(\n",
            "              (DenseReluDense): T5DenseGatedActDense(\n",
            "                (wi_0): Linear(in_features=1472, out_features=3584, bias=False)\n",
            "                (wi_1): Linear(in_features=1472, out_features=3584, bias=False)\n",
            "                (wo): Linear(in_features=3584, out_features=1472, bias=False)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "                (act): NewGELUActivation()\n",
            "              )\n",
            "              (layer_norm): T5LayerNorm()\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (final_layer_norm): T5LayerNorm()\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (fc3): Linear(in_features=1472, out_features=3000, bias=True)\n",
            ")\n",
            "INFO:root:setting learning rate to 0.001\n",
            "setting learning rate to 0.001\n",
            "  0% 0/1500 [00:00<?, ?it/s]INFO:root:Epoch 0 training loss 8.008275032043457\n",
            "Epoch 0 training loss 8.008275032043457\n",
            "INFO:root:Epoch 0 eval loss 8.003212571144104  accuracy 0.0 true distance avg 8287.892578125 true distance median 7496.5576171875\n",
            "Epoch 0 eval loss 8.003212571144104  accuracy 0.0 true distance avg 8287.892578125 true distance median 7496.5576171875\n",
            "INFO:root:saved to models/byt5-class-0\n",
            "saved to models/byt5-class-0\n",
            "  1% 11/1500 [00:33<35:46,  1.44s/it]"
          ]
        }
      ],
      "source": [
        "\n",
        "# Run the training script\n",
        "# The parameters here are chosen to show a small training run on a small subset of data\n",
        "!python byt5-geotagging/train_model.py --train_input_file train.csv --test_input_file test.csv --do_train true --do_test true --load_clustering ./ --device cuda --batch_size 64 --keep_layer_count 4 --max_train 96000 --max_test 640\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDY5kaY7hhZl"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Run the testing script\n",
        "# For demo purpose, a small subset of validation set is used\n",
        "!python byt5-geotagging/train_model.py --train_input_file train.csv --test_input_file test.csv --do_test true --load_clustering ./ --load_model_dir models/byt5-class-0 --device cuda --batch_size 32  --max_train 96000 --max_test 640\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDzMsI92htqN"
      },
      "outputs": [],
      "source": [
        "cd byt5-geotagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRMdQXHEhiRA"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load trained model and tokenizer\n",
        "device = 'cuda'\n",
        "byt5 = torch.load('../models/byt5-class-0')\n",
        "byt5_tokenizer = AutoTokenizer.from_pretrained('google/byt5-small')\n",
        "\n",
        "# Make a prediction\n",
        "text = 'I live in New York'\n",
        "inputs = byt5_tokenizer(text, return_tensors='pt')['input_ids'].unsqueeze(0)\n",
        "logits = byt5.to(device)(inputs.to(device))\n",
        "predicted_cluster = logits.argmax()\n",
        "confidence = torch.nn.functional.softmax(logits, dim=-1).max().item()\n",
        "predicted_location = cluster_df.iloc[predicted_cluster.item()]\n",
        "\n",
        "# Output predicted location and confidence\n",
        "print(predicted_location['lat'], predicted_location['lng'], confidence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljHoXvAOilvO"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}